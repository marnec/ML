{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "broken-cloud",
   "metadata": {},
   "source": [
    "---\n",
    "layout: default\n",
    "title: \"Deep Learning - RNN - Word embeddings\"\n",
    "categories: deeplearning\n",
    "permalink: /ML45/\n",
    "order: 45\n",
    "comments: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "controversial-dallas",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab --no-import-all inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"cm\"\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "from mpl_flow import Flow\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-being",
   "metadata": {},
   "source": [
    "# Word embeddings in NLP\n",
    "One of the field of machine learning being revolutionized by RNN is Natural Language Processing (NLP), a classically complex task due to the very changeling nature of language and the nuances in its meaning. One the key concepts helping with NLP-related tasks is **word embeddings**, a representation for words that let an algorithm learn analogies (e.g. man is to woman, as king is to queen).\n",
    "\n",
    "## Word representation\n",
    "Up until this point we have been representing words with a vocabulary vector $V$ of a fixed size (let's say 10,000 words) with a word represented by a one-hot vector of size $|V|$\n",
    "\n",
    "$$\n",
    "V=\\begin{bmatrix}\n",
    "\\small\\text{a}\\\\\n",
    "\\small\\text{aaron} \\\\\n",
    "\\vdots \\\\\n",
    "\\small\\text{zulu}\\\\\n",
    "\\small\\text{<UNK>}\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "\\text{Man}=\n",
    "\\begin{bmatrix}\n",
    "\\small\\text{0}\\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where we would represent the word `Man` as $O_{5391}$ and the word `Woman` as $O_{9853}$ if these words are at position 5391 and 9853 respectively. This representation has the weakness of treating each word as a separate entity not allowing an algorithm to easily generalize across words. For example suppose we have the two sentences with a blank space:\n",
    "\n",
    "> I want a glass of orange ____\n",
    "\n",
    "> I want a glass of apple ____\n",
    "\n",
    "We can easily see that a word that fits well both blank spaces is `juice`. However, an algorithm that has learned that *I want a glass of **orange** juice* is a likely sentence from the distribution of sentences in our corpus, doesn't necessarily have a similar likelihood for the sentence *I want a glass of **apple** juice*. As far as it knows, the relationship between the words *Apple* and *Orange* is not any closer than the relationship between the words *Apple* and *Man*. In fact the inner product and euclidean distance between any two one-hot vectors is zero since they are all orthogonal.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "& \\langle O_x, O_y \\rangle = 0 \\\\\n",
    "& O_x - O_y = 0\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Behind the concept of word embedding, the one-hot representation is replaced by a featurized representation where each word is represented by a set of **learned** features. This new representation is called **embedding**.\n",
    "\n",
    "|          | Man <br>(5391) | Woman <br>(9853) | King <br>(4914) | Queen <br>(7157)| Apple <br>(456) | Orange<br>(6257) |\n",
    "|----------|------|-------|-------|-------|-------|--------|\n",
    "| Gender   | -1   | 1     | -0.95 | 0.97  | 0     | 0.01   |\n",
    "| Royal    | 0.01 | 0.02  | 0.93  | 0.95  | -0.01 | 0.00   |\n",
    "| Age      | 0.03 | 0.02  | 0.7   | 0.69  | 0.03  | -0.02  |\n",
    "| Food     | 0.04 | 0.01  | 0.02  | 0.01  | 0.95  | 0.97   |\n",
    "| $\\vdots$ |      |       |       |       |       |        |\n",
    "\n",
    "We can now notice that the representation for *Apple* $e_{456}$ and for the word *Orange* $e_{6257}$ are very similar. We expect some of the features to be different but in general their representation should be consistently close. This increases the odds of a learning algorithm to generalize the probability associated to a sentence containing the word *Orange* to the same sentence with the word *Apple*. Learned features are not easily interpretable as the ones used in this example and their exact representation is often hard to figure out.\n",
    "\n",
    "### Visualizing word embeddings\n",
    "Once a high-dimensional featurized representation (embedding) is built for each word in a vocabulary, each word will be represented by a high-dimensional vector of feature components. This reflects the reason why they are called embeddings: they are imagined as points *embedded* in a high-dimensional feature-space. It could be useful to visualize the embeddings but it is impossible to represent more than 2-3 dimensions in a plot. To visualize them the high-dimensional space is compressed to a 2D space. The compression method commonly used is the **t-SNE** algorithm ([van der Maaten and Hinton, 2008](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
