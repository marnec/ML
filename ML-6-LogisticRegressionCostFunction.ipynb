{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: default\n",
    "title: \"Logistic Regression - Cost function\"\n",
    "categories: logisticRegression\n",
    "permalink: /ML6/\n",
    "order: 6\n",
    "comments: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%pylab --no-import-all inline\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "# Cost Function in Logistic Regression\n",
    "Given a training set of $m$ examples\n",
    "\n",
    "$$\n",
    "{(x^{1}, y^{1}),(x^{2}, y^{2}), \\dots, (x^{m}, y^{m})}\n",
    "$$\n",
    "\n",
    "where $x$ is a vector $\\mathbb{R}^{n+1}$ with $x_0=1$ and $y$ assumes discrete values $0,1$\n",
    "\n",
    "$$\n",
    "x \\in\n",
    "\\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\\n",
    "\\dots\\\\\n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$y \\in \\{0,1\\}$$\n",
    "\n",
    "and the hypothesis $h_\\theta(x)$ is logistic\n",
    "\n",
    "$$h_\\theta(x)=\\frac{1}{1+e^{-\\theta^Tx}}$$\n",
    "\n",
    "How do we chose (or how do we fit) the parameters $\\theta$? For Linear regression we use the Cost function $J(\\theta)$\n",
    "\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum^m_{i=1}\\frac{1}{2}\\left(h_\\theta\\left(x^{(i)}\\right)-y^{(i)}\\right)^2$$\n",
    "\n",
    "But this function used is logistic regression is **non-convex**, it has many local minima and using gradient descent would lead to poor optimization (minimzation) and is not guaranteed to converge to the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "cycles = 8\n",
    "p = np.linspace(-np.pi*cycles, np.pi*cycles, 1000)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(p, p**2, label='convex')\n",
    "ax.plot(p, np.sin(p)*100+p**2, label='non convex')\n",
    "ax.set_xlabel('$\\\\theta$')\n",
    "ax.set_ylabel('$J(\\\\theta)$')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cost function for a single training example is called the **Loss function** $\\mathcal{L}$\n",
    "\n",
    "$$\\frac{1}{2}\\left(h_\\theta\\left(x^{(i)}\\right)-y^{(i)}\\right)^2 \\equiv \\mathcal{L} \\implies J(\\theta)=\\frac{1}{m}\\sum_{i=1}^m\\mathcal{L}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "For a single training example, the Cost function is called the **Loss function** $\\mathcal{L}$\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}=\n",
    "\\begin{cases}\n",
    "-log(h_\\theta(x)), & \\text{if } y=1\\\\\n",
    "-log(1-h_\\theta(x)), & \\text{if } y=0\\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(1,2, figsize=(12,5))\n",
    "hx=np.linspace(0.01,.99)\n",
    "x1=np.linspace(0.1,1)\n",
    "x2=np.linspace(1,2)\n",
    "ax1.set_title('$y=1$')\n",
    "ax1.plot(hx, -np.log(hx))\n",
    "ax1.set_xlabel('$h_\\\\theta(x)$')\n",
    "ax1.set_ylabel('Cost')\n",
    "ax1.set_xticks([0,1])\n",
    "ax1.set_yticks([0])\n",
    "\n",
    "ax1i = inset_axes(ax1, width=2, height=1.5)\n",
    "ax1i.set_xticks([0,1])\n",
    "ax1i.set_yticks([])\n",
    "ax1i.spines['bottom'].set_position('center')\n",
    "ax1i.spines['right'].set_visible(False)\n",
    "ax1i.spines['top'].set_visible(False)\n",
    "ax1i.plot(np.r_[x1,x2], np.log(np.r_[x1,x2]), c='C1', label='$\\log(z)$')\n",
    "ax1i.plot(x1, -np.log(x1), label='$-\\log(z)$')\n",
    "ax1i.plot(x2, -np.log(x2), ls='--', c='C0')\n",
    "ax1i.legend( bbox_to_anchor=(1, 0), loc='upper right')\n",
    "\n",
    "\n",
    "ax2.set_title('$y=0$')\n",
    "ax2.set_xticks([0,1])\n",
    "ax2.set_yticks([0])\n",
    "ax2.plot(hx, -np.log(1-hx))\n",
    "ax2i = inset_axes(ax2, width=2, height=1.5, loc='upper left',\n",
    "                  bbox_to_anchor=(1.3,1.3), bbox_transform=ax.transAxes)\n",
    "\n",
    "ax2i.set_xticks([0,1])\n",
    "ax2i.set_yticks([])\n",
    "# ax1i.spines['bottom'].set_position('center')\n",
    "ax2i.spines['right'].set_visible(False)\n",
    "ax2i.spines['top'].set_visible(False)\n",
    "ax2i.plot(x1, -np.log(1.01-x1), label='$-\\log(1-z)$')\n",
    "ax2i.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cost function has some desirable properties:\n",
    "\n",
    "* For $y=1$\n",
    "    * $h_\\theta(x)\\to1 \\implies \\mathcal{L}\\to0$. This is what we want because there should be an increasingly smaller cost when $h_\\theta(x)\\to y$ and no cost when $h_\\theta(x) = y = 1$\n",
    "\n",
    "    * $h_\\theta(x) \\to 0 \\implies \\mathcal{L}\\to\\infty$. This captures the intuition that if $y=1$ and $P(y=1\\mid x;\\theta)$ the algorithm is penalized by a large cost\n",
    "    \n",
    "* For $y=0$\n",
    "    * $h_\\theta(x)\\to1 \\implies \\mathcal{L}\\to\\infty$. There is a big cost associated to predicting 1 when $y=0$\n",
    "    * $h_\\theta(x)\\to0 \\implies \\mathcal{L}\\to0$. There is no cost associated to predicting 0 when $y=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified cost function\n",
    "Since $y\\in{0,1}$ we can write the $\\mathcal{L}$ function in a simpler way and compress the two cases in one equation.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(h_\\theta(x),y)=-y\\log(h_\\theta(x))-(1-y)\\log(1-h_\\theta(x))\n",
    "$$\n",
    "\n",
    "When $y=1$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(h_\\theta(x),y)&=-1\\cdot\\log(h_\\theta(x))-0\\cdot\\log(1-h_\\theta(x))\\\\\n",
    "&=-\\log(h_\\theta(x))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "When $y=0$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(h_\\theta(x),y)&=-0\\cdot\\log(h_\\theta(x))-1\\cdot\\log(1-h_\\theta(x))\\\\\n",
    "&=-\\log(1-h_\\theta(x))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "Now that we have a more compact way of writing the cost function we can write it for the whole dataset\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta)&=\\frac{1}{m}\\sum_{i=1}^m\\mathcal{L}\\left(h_\\theta \\left(x^{(i)} \\right),y^{(i)}\\right)\\\\\n",
    "&=-\\frac{1}{m}\\left[\\sum_{i=1}^my^{(i)}\\log\\left(h_\\theta(x^{(i)})\\right)+\\left(1-y^{(i)}\\right)\\log\\left(1-h_\\theta(x^{(i)})\\right)\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Although there are other cost functions that can be used this cost function can be derived from statistics using the principle of [maximum likelihood estimation.](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)\n",
    "\n",
    "In order to estimate the parameters with this cost function we have to find the parameters $\\theta$ that minimize $J(\\theta)$\n",
    "\n",
    "# Gradient descent\n",
    "To minimize $J(\\theta)$ we are going to use gradient descent\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\theta_j := \\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\n",
    "\\end{equation*}\n",
    "\\label{eq:gdescent} \\tag{1}\n",
    "$$\n",
    "\n",
    "Where we repeat $\\eqref{eq:gdescent}$ for all element $\\theta_j$ of the parameters vector $\\theta$ updating the parameters simoultanueously (after calculating them).\n",
    "\n",
    "Deriving the term $\\theta_j$ we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\theta_j}=\\frac{1}{m}\\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_j^{(i)}\n",
    "$$\n",
    "\n",
    "Plugging the derived term into the gradient descent we obtain\n",
    "\n",
    "$$\\theta_j := \\theta_j-\\frac{\\alpha}{m}\\sum_{i=1}^m\\left(h_\\theta(x^{(i)})-y^{(i)}\\right)x_j^{(i)}$$\n",
    "\n",
    "This looks identical to gradient descent for linear regression, however the definition of $h_\\theta(x)$ is changed and is now $\\frac{1}{1+e^{\\theta^Tx}}$\n",
    "\n",
    "# Vectorization\n",
    "Vectorized implementations of the cost function and the gradient descent are\n",
    "\n",
    "## Cost function\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&h = g(\\theta x)\\\\\n",
    "&J(\\theta)=\\frac{1}{m}\\left(-y^T\\log(h)-(1-y)^T\\log(1-h)\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "$$\n",
    "\\theta:=\\theta-\\frac{\\alpha}{m}X^T\\left(g(X\\theta)-\\vec{y}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Optimization\n",
    "By applying some concepts of optimization, we can fit logistic regression parameters much more efficiently than gradient descent and make the logistic regression algorithm scale better for large datasets.\n",
    "\n",
    "Until now we have chosen to use the gradient descent optimization algorithm. However, there are other, more sophisticated optimization algorithms:\n",
    "\n",
    "* [Conjugate descent](https://en.wikipedia.org/wiki/Conjugate_gradient_method)\n",
    "* [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)\n",
    "* [L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)\n",
    "\n",
    "These algorithms, at the cost of being more complex, share a series of advantages:\n",
    "\n",
    "* They remove need of manually picking an $\\alpha$\n",
    "* They are often faster than gradient descent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
