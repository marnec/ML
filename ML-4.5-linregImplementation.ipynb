{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "related-allah",
   "metadata": {},
   "source": [
    "---\n",
    "layout: default\n",
    "categories: linearRegression\n",
    "title: \"Linear Regression - Implementation\"\n",
    "permalink: /ML4.5/\n",
    "order: 4.5\n",
    "comments: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-forty",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%pylab --no-import-all inline\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"cm\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-plane",
   "metadata": {},
   "source": [
    "# Linear regression implementation\n",
    "Since linear regression is a trivial model, it is relatively easy to implement it from scratches and maybe in the future I'll implement a full version on this page. \n",
    "\n",
    "Many libraries enabling a user to build and train a linear regression model exist. In the last years I feel like `scikit-learn` and `pytorch` are the most widely used libraries in machine learning.\n",
    "\n",
    "## Reading data\n",
    "For this example we are using house prices as a function of inhabitable surface and number of rooms. Data is stored in a csv file, to parse it into a python data structure we use `pandas`. This is a preliminary step for any approach and while some libraries may offer custom way to parse data I find that this is just better. Delegating parsing to a second library follows the *single-responsibility* principle. This is at least true for datasets saved in common formats like `csv` or `tsv` or similar. Sometimes we will deal with custom formats like Pytorch's `pt` files: in that case it is obviously better (or sometimes necessary) to take care of data loading with the right library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-lounge",
   "metadata": {},
   "source": [
    "We read data from a `csv` file and cast it into a `pandas.DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/house_pricing.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-secondary",
   "metadata": {},
   "source": [
    "This dataset has two feature columns (`sqf` and `rooms`) and a label column (`price`)\n",
    "\n",
    "Let's assign the features $X$ and the labels $y$ to two different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = df.values.T\n",
    "X = xy[:-1].T\n",
    "y = xy[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-costs",
   "metadata": {},
   "source": [
    "Where the features $X$ are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-investigator",
   "metadata": {},
   "source": [
    "and their labels $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:5].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-correspondence",
   "metadata": {},
   "source": [
    "## scikit-learn\n",
    "Linear regression in `scikit-learn` is as easy as one line of code. To keep this first example as easy as possible, I'm not going to split the data in training and dev sets. I'm just fitting the model to the whole dataset. In a real scenario, there should be a preliminary step of dataset splitting. \n",
    "\n",
    "### Single feature\n",
    "In order for the first example to be as simple as possible and plottable, for now we drop the `rooms` column from the features and we are only left with the `sqf` column. This means that in this first example we are exploring linear dependency between the inhabitable surface and the price of a house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple = X[:, 0]\n",
    "X_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-match",
   "metadata": {},
   "source": [
    "Since the `fit()` function that we are using later wants a 2D-vector of shape $(m, n)$  and we only have one feature, we need to reshape the array in the form $(m, 1)$. On the other hand $y$ can either be a 2D or 1D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple = X_simple.reshape(-1, 1)\n",
    "X_simple[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-aggregate",
   "metadata": {},
   "source": [
    "Building a linear regression model with [scikit-learn](https://scikit-learn.org/) requires the `LinearRegression` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-profile",
   "metadata": {},
   "source": [
    "Now we can build the model buy instantiating the `LinearRegression` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-identifier",
   "metadata": {},
   "source": [
    "the `linreg` variable contains a linear regression object that allow the computation of the model, but we didn't feed the data to it. Data is fed to the `.fit()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = linreg.fit(X_simple, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-cylinder",
   "metadata": {},
   "source": [
    "The parameters and bias of the model are returned with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.coef_, linreg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-cornell",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X_simple, y, ls='none', marker='o')\n",
    "px = np.linspace(1000, 5000)\n",
    "ax.plot(px, linreg.predict(px.reshape(-1, 1)))\n",
    "ax.set_title(f'$\\\\hat{{y}}={linreg.coef_[0]:.2f} \\cdot x '\n",
    "             f'+ {linreg.intercept_:.2f}$', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-franchise",
   "metadata": {},
   "source": [
    "### Multiple Features\n",
    "We can now introduce the dataset split step that we oversaw in the previous example. In `scikit-learn` splitting the dataset in train and test set is taken care of for us through a function. The proportion of the split can be configured through its arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-vienna",
   "metadata": {},
   "source": [
    "We can now fit the all ($n=2$) features of the training set $X^t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-xerox",
   "metadata": {},
   "source": [
    "Since this time $X^t \\in \\mathbb{R}^{m \\times 2}$, we have 2 weight parameters and 1 bias parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.coef_, linreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-redhead",
   "metadata": {},
   "source": [
    "Parameters fitted on the training set can be used to produce prediction from the test set features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linreg.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-washer",
   "metadata": {},
   "source": [
    "Predictions can be now compared to the labels of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "explained_variance_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-accommodation",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "Whereas `scikit-learn` is a high-level library, `Pytorch` is has a much lower-level approach. Many of the things that in `scikit-learn` happen under the hoods, in `Pytorch` need to be done manually.\n",
    "\n",
    "The steps for training a model in Pytorch as defined in [Pytorch documentation](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py) are\n",
    "\n",
    "1. Load Dataset\n",
    "2. Make Dataset Iterable\n",
    "3. Create Model Class\n",
    "4. Instantiate Model Class\n",
    "5. Instantiate Loss Class\n",
    "6. Instantiate Optimizer Class\n",
    "7. Train Model\n",
    "\n",
    "The main entry point of the framework is the `torch` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-swiss",
   "metadata": {},
   "source": [
    "The first noticeable Pytorch feature is that it works using a proprietary data-structure, called a `tensor`. The underlying mathematical concept of tensor is beyond the scope of this article but can be consulted at the [Wikipedia tensor entry](https://en.wikipedia.org/wiki/Tensor). In Pytorch, a `tensor` is (citing the [pytorch tensor documentation](https://pytorch.org/docs/stable/tensors.html)) *a multi-dimensional matrix containing elements of a single data type*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "X_tensor[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-alfred",
   "metadata": {},
   "source": [
    "As you can notice we had to specify `dtype=np.float32`. This is because the underlying implementation of forward and backward propagation used by Pytorch under the hood would not work with the `int` type.\n",
    "\n",
    "Furthermore, $y$ tensor would be 1D but this would not comply with requirements of Pytorch methods used below, so we transform it into a column vector with the `unsqueeze(-1)` method. This is equivalent to calling `.reshape(-1, 1)` on a `numpy.array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)\n",
    "y_tensor[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-people",
   "metadata": {},
   "source": [
    "Since data is in very different scales we need to first normalize it. Here we use [standardization](https://en.wikipedia.org/wiki/Standard_score), which rescales data to have mean $\\mu=0$ and standard deviation $\\sigma=1$\n",
    "\n",
    "$$\n",
    "X_\\text{std} = \\frac{X - \\mu}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor_norm = (X_tensor - X_tensor.mean()) / torch.sqrt(X_tensor.var())\n",
    "X_tensor_norm[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-realtor",
   "metadata": {},
   "source": [
    "A linear regression model can be built using the `Linear` class from the `nn` module, which initializes bias and weights automatically. Its constructor takes as input the number of columns of the input ($n_X$) and of the output ($n_y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-welding",
   "metadata": {},
   "source": [
    "Training the model will require some hyperparameters that we will define in advance for convenience:\n",
    "\n",
    "* `epochs` is the number of times the model will see all of our training samples;\n",
    "* `alpha` is the learning rate, which defines how big are the steps takes in updating the parameters;\n",
    "* `loss_func` is the loss function $\\mathcal{L}$ used at training time. In this case we are using the `MSELoss`, which measures the mean squared error (squared L2 norm) between each element in the input $x$ and target $y$;\n",
    "* `optim` is the optimization algorithm used. In this case we are using `SGD` (Stochastic Gradient Descent). SGD requires us to select the correct $\\alpha$. Up to this point we haven't seen that other optimization algorithm exist that automatically adapt $\\alpha$ to data (e.g. [ADAM](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam)), so we are sticking with standard SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "alpha = 0.01\n",
    "loss_func = torch.nn.MSELoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-dover",
   "metadata": {},
   "source": [
    "Now we need to manually run over the epochs and trigger the update of the parameters that will ultimately produce a fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    inputs = Variable(X_tensor_norm)\n",
    "    labels = Variable(y_tensor)\n",
    "    # Clear gradient buffers because from previous epoch\n",
    "    optim.zero_grad()\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "    # get loss for the predicted output\n",
    "    loss = loss_func(outputs, labels)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "    # update parameters\n",
    "    optim.step()\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(inputs)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-concern",
   "metadata": {},
   "source": [
    "## Pytorch-lightning\n",
    "Pytorch-lightning is a high-level wrapper for Pytorch that at the same time prevents you from writing much of the boilerplate code needed for a Pytorch model, and automatically adopts the most suited optimization strategies.\n",
    "\n",
    "It works on top of Pytorch-lightning and scikit-learn (among the others) and really speeds up the design process. For example its additional module `lightning bolts` offers linea regression and logistic regression implementations with `numpy` and `sklearn` bridges for datasets! But their implementations work on multiple GPUs, TPUs and scale dramatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pl_bolts.models.regression import LinearRegression\n",
    "# import pytorch_lightning as pl\n",
    "# from pl_bolts.datamodules import SklearnDataModule\n",
    "\n",
    "# loaders = SklearnDataModule(X_tensor_norm.numpy(), y_tensor.numpy(), num_workers=4)\n",
    "# model = LinearRegression(input_dim=2, bias=True)\n",
    "# trainer = pl.Trainer()\n",
    "# trainer.fit(model, train_dataloader=loaders.train_dataloader(), val_dataloaders=loaders.val_dataloader())\n",
    "# trainer.test(test_dataloaders=loaders.test_dataloader())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
