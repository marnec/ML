{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656c6dba-a799-4948-9883-152a74577735",
   "metadata": {},
   "source": [
    "---\n",
    "layout: default\n",
    "categories: CNN\n",
    "title: \"CNN - Implementation\"\n",
    "permalink: /ML34.5/\n",
    "order: 34.5\n",
    "comments: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6865ea5-24f5-4b01-8da2-08735941d114",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "%pylab --no-import-all inline\n",
    "import matplotlib_inline.backend_inline\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"cm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd57bacf-571c-4e9f-bcc0-21d73083a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d2bbdf-6ae0-42c7-8086-79d08090fa63",
   "metadata": {},
   "source": [
    "Convolutional neural networks (CNN) are usually used fot computer vision tasks and current implementations of famous architectures are much more complex than what we can easily write in few lines of code as up until now.\n",
    "\n",
    "Luckily a simlpe CNN can be implemented by just adding a couple of layers to the architecture of an FNN  that we built in <a href=\"page:ML17.5\">ML-17.5</a>\n",
    "\n",
    "First we are going to use the same MNIST dataset as for the other examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f0bb62-b88e-4929-adcb-e6f6b419fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(\n",
    "    root=\"./data\", train=True, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "\n",
    "test_dataset = dsets.MNIST(root=\"./data\", train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a9308-ddeb-455b-8513-e1ce8777da36",
   "metadata": {},
   "source": [
    "We have a training set of 60000 vectors $x^{(i)} \\in \\mathbb{R}^{784}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d53a9269-9a4f-4026-a566-ae1645cc7399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec054c2d-2dc2-4d60-8500-ff4835c8bd41",
   "metadata": {},
   "source": [
    "And 60000 traning labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3104fb79-711b-49eb-91e3-22ec01bdbb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.targets.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370b669-43d0-4bb2-a9ef-4a2997a8f53c",
   "metadata": {},
   "source": [
    "And a test set of 10000 images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39a8b93e-fc4d-4239-a7ca-ccaf9c9c9841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.targets.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbaf6e5-6c25-4e68-bf53-7ff31bd5c22a",
   "metadata": {},
   "source": [
    "With a mini-batch size of 100 images and a total number of 3000 iterations, we will go through the whole dataset 5 times (5 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd73b7ad-ab52-4e79-8475-e61ebee44a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = int(n_iters / (len(train_dataset) / batch_size))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dd982c-5758-450a-ad74-5f61dd1dadb8",
   "metadata": {},
   "source": [
    "The model architecture will have:\n",
    "\n",
    "1. A Convolutional layer with *Same Padding*\n",
    "2. A *Max Pooling* layer\n",
    "3. A Convolutional layer with *Same Padding*\n",
    "4. A *Max Pooling* layer\n",
    "5. A fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aded8943-89b9-43ca-82f3-987373201e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(\n",
    "            in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Fully connected 1 (readout)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "\n",
    "        # Convolution 2\n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Max pool 2\n",
    "        out = self.maxpool2(out)\n",
    "\n",
    "        # Resize\n",
    "        # Original size: (100, 32, 7, 7)\n",
    "        # out.size(0): 100\n",
    "        # New out size: (100, 32*7*7)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfe48fcf-8a06-4701-a66a-3bd261906d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6b8b5f0-39de-4c68-9553-13d9bd6fc631",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6e194a6-4816-4e8c-81ee-2c3113402424",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b8bef51-807f-47a3-8860-767858764afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "torch.Size([16, 1, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([32, 16, 5, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([10, 1568])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(len(list(model.parameters())))\n",
    "\n",
    "# Convolution 1: 16 Kernels\n",
    "print(list(model.parameters())[0].size())\n",
    "\n",
    "# Convolution 1 Bias: 16 Kernels\n",
    "print(list(model.parameters())[1].size())\n",
    "\n",
    "# Convolution 2: 32 Kernels with depth = 16\n",
    "print(list(model.parameters())[2].size())\n",
    "\n",
    "# Convolution 2 Bias: 32 Kernels with depth = 16\n",
    "print(list(model.parameters())[3].size())\n",
    "\n",
    "# Fully Connected Layer 1\n",
    "print(list(model.parameters())[4].size())\n",
    "\n",
    "# Fully Connected Layer Bias\n",
    "print(list(model.parameters())[5].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40b6dcc1-3b07-4827-afe3-739cedc08461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.5105285048484802. Accuracy: 87.93000030517578\n",
      "Iteration: 1000. Loss: 0.2870190441608429. Accuracy: 93.08999633789062\n",
      "Iteration: 1500. Loss: 0.13547983765602112. Accuracy: 94.5199966430664\n",
      "Iteration: 2000. Loss: 0.08501622080802917. Accuracy: 95.7300033569336\n",
      "Iteration: 2500. Loss: 0.15075716376304626. Accuracy: 96.43000030517578\n",
      "Iteration: 3000. Loss: 0.11706560850143433. Accuracy: 96.93000030517578\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images\n",
    "        images = images.requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images\n",
    "                images = images.requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print(\n",
    "                \"Iteration: {}. Loss: {}. Accuracy: {}\".format(\n",
    "                    iter, loss.item(), accuracy\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6adf503-2e10-427f-8106-8cd5c9a5f67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.33858054876327515. Accuracy: 86.51000213623047\n",
      "Iteration: 1000. Loss: 0.31930989027023315. Accuracy: 89.54000091552734\n",
      "Iteration: 1500. Loss: 0.37224602699279785. Accuracy: 90.75\n",
      "Iteration: 2000. Loss: 0.14266057312488556. Accuracy: 91.61000061035156\n",
      "Iteration: 2500. Loss: 0.1566978245973587. Accuracy: 92.36000061035156\n",
      "Iteration: 3000. Loss: 0.32563263177871704. Accuracy: 93.25\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\"\"\"\n",
    "STEP 1: LOADING DATASET\n",
    "\"\"\"\n",
    "\n",
    "train_dataset = dsets.MNIST(\n",
    "    root=\"./data\", train=True, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "\n",
    "test_dataset = dsets.MNIST(root=\"./data\", train=False, transform=transforms.ToTensor())\n",
    "\n",
    "\"\"\"\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Average pool 1\n",
    "        self.avgpool1 = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(\n",
    "            in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Average pool 2\n",
    "        self.avgpool2 = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "        # Fully connected 1 (readout)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Average pool 1\n",
    "        out = self.avgpool1(out)\n",
    "\n",
    "        # Convolution 2\n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Max pool 2\n",
    "        out = self.avgpool2(out)\n",
    "\n",
    "        # Resize\n",
    "        # Original size: (100, 32, 7, 7)\n",
    "        # out.size(0): 100\n",
    "        # New out size: (100, 32*7*7)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "\"\"\"\n",
    "\n",
    "model = CNNModel()\n",
    "\n",
    "\"\"\"\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "\"\"\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "\"\"\"\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\"\"\"\n",
    "STEP 7: TRAIN THE MODEL\n",
    "\"\"\"\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as tensors with gradient accumulation abilities\n",
    "        images = images.requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to tensors with gradient accumulation abilities\n",
    "                images = images.requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print(\n",
    "                \"Iteration: {}. Loss: {}. Accuracy: {}\".format(\n",
    "                    iter, loss.item(), accuracy\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f818f479-1c3e-4aec-ace0-a13b8ce9a998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
