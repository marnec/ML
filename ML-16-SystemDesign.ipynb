{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: default\n",
    "title: \"System design - numerical evaluation\"\n",
    "categories: design\n",
    "permalink: /ML16/\n",
    "order: 16\n",
    "comments: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab --no-import-all inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning system design\n",
    "What are the decision that we have to take to effectively and efficiently train a machine learning algorithm? Suppose we want to build a classifier that can discriminate between spam and legitimate emails.\n",
    "\n",
    "How do we choose the features for our emails?\n",
    "\n",
    "There is not really a way to tell in advance if you'll need complex features, more data or anything else so a good approach is usually to take these 3 steps:\n",
    "\n",
    "* start with a simple algorithm that can be implemented quickly, implement it and test it on the cross-validation set;\n",
    "* plot learning curves to decide what is more likely to help;\n",
    "* error analysis: manually examine the examples in the cross validation set that your algorithm made errors on: identifying possible systematic errors can help to detect current shortcomings of the system.\n",
    "\n",
    "Let's say that you have built spam-email classifier and you have $m_\\text{CV}=500$ examples in the cross validation set and the algorithm misclassifies 100 emails. Following the suggestions above you would manually examine the 100 misclassified email and categorize them based on:\n",
    "\n",
    "* type of email\n",
    "* what feature(s) would have helped correctly classify the email\n",
    "\n",
    "What we want to achieve is understanding what examples our algorithm finds difficult and what features is worth to spend time on. Very often different algorithm will have difficulties on the same set of examples so starting from a quick and simple algorithm may save you quite a lot of time and pointing you in good directions about what is worth prioritizing.\n",
    "\n",
    "## The importance of numerical evaluation\n",
    "When evaluating a learning algorithm it is very helpful to have a proxy of the performance of your algorithm in a single number.\n",
    "\n",
    "Suppose you have to decide if including [stemming](https://en.wikipedia.org/wiki/Stemming) as a feature of your spam-email classifier. manually looking at wrong examples in the cross validation set may not be a good way to determine if including stemming improves performance. By looking at the percentage of errors on the cross-validation set will instead give you immediately an idea of how much a certain operation impacts on the algorithm performance.\n",
    "\n",
    "Sometimes looking at the errors is not a good proxy of performance of a learning algorithm and more sophisticated measures are used. This will be expanded upon on later articles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
