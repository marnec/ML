```python
import torch
import torch.nn as nn
import torch.nn.functional as F
```


```python
import numpy as np
from collections import Counter
import os
from argparse import Namespace
```


```python
flags = Namespace(
    train_file='data/newtestament.txt',
    seq_size=32,
    batch_size=16,
    embedding_size=64,
    lstm_size=64,
    gradients_norm=5,
    initial_words=['I', 'am'],
    predict_top_k=5,
    checkpoint_path='checkpoint',
)
```


```python
def get_data_from_file(train_file, batch_size, seq_size):
    with open(train_file, 'r') as f:
        text = f.read()
    text = text.split()

    word_counts = Counter(text)
    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)
    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}
    vocab_to_int = {w: k for k, w in int_to_vocab.items()}
    n_vocab = len(int_to_vocab)

    print('Vocabulary size', n_vocab)

    int_text = [vocab_to_int[w] for w in text]
    num_batches = int(len(int_text) / (seq_size * batch_size))
    in_text = int_text[:num_batches * batch_size * seq_size]
    out_text = np.zeros_like(in_text)
    out_text[:-1] = in_text[1:]
    out_text[-1] = in_text[0]
    in_text = np.reshape(in_text, (batch_size, -1))
    out_text = np.reshape(out_text, (batch_size, -1))
    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text
```


```python
def get_batches(in_text, out_text, batch_size, seq_size):
    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)
    for i in range(0, num_batches * seq_size, seq_size):
        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]
```


```python
class RNNModule(nn.Module):
    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):
        super(RNNModule, self).__init__()
        self.seq_size = seq_size
        self.lstm_size = lstm_size
        self.embedding = nn.Embedding(n_vocab, embedding_size)
        
        self.lstm = nn.LSTM(embedding_size,
                            lstm_size,
                            batch_first=True)
        
        self.dense = nn.Linear(lstm_size, n_vocab)
    
    def forward(self, x, prev_state):
        embed = self.embedding(x)
        output, state = self.lstm(embed, prev_state)
        logits = self.dense(output)

        return logits, state
    
    def zero_state(self, batch_size):
        return (torch.zeros(1, batch_size, self.lstm_size),
                torch.zeros(1, batch_size, self.lstm_size))
```


```python
def get_loss_and_train_op(net, lr=0.001):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)

    return criterion, optimizer
```


```python
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(
        flags.train_file, flags.batch_size, flags.seq_size)

    net = RNNModule(n_vocab, flags.seq_size,
                    flags.embedding_size, flags.lstm_size)
    net = net.to(device)

    criterion, optimizer = get_loss_and_train_op(net, 0.01)

    iteration = 0
    
    for e in range(50):
        batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)
        state_h, state_c = net.zero_state(flags.batch_size)

        # Transfer data to GPU
        state_h = state_h.to(device)
        state_c = state_c.to(device)
        for x, y in batches:
            iteration += 1

            # Tell it we are in training mode
            net.train()

            # Reset all gradients
            optimizer.zero_grad()

            # Transfer data to GPU
            x = torch.tensor(x).to(device)
            y = torch.tensor(y).to(device)

            logits, (state_h, state_c) = net(x, (state_h, state_c))
            loss = criterion(logits.transpose(1, 2), y)

            state_h = state_h.detach()
            state_c = state_c.detach()

            loss_value = loss.item()

            # Perform back-propagation
            loss.backward(retain_graph=True)
            
            
            _ = torch.nn.utils.clip_grad_norm_(
                net.parameters(), flags.gradients_norm)

            # Update the network's parameters
            optimizer.step()


            if iteration % 100 == 0:
                print('Epoch: {}/{}'.format(e, 200),
                      'Iteration: {}'.format(iteration),
                      'Loss: {}'.format(loss_value))

            if iteration % 1000 == 0:
                predict(device, net, flags.initial_words, n_vocab,
                        vocab_to_int, int_to_vocab, top_k=5)
                torch.save(net.state_dict(),
                           'checkpoint_pt/model-{}.pth'.format(iteration))
```


```python
def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):
    net.eval()

    state_h, state_c = net.zero_state(1)
    state_h = state_h.to(device)
    state_c = state_c.to(device)
    for w in words:
        ix = torch.tensor([[vocab_to_int[w]]]).to(device)
        output, (state_h, state_c) = net(ix, (state_h, state_c))
    
    _, top_ix = torch.topk(output[0], k=top_k)
    choices = top_ix.tolist()
    choice = np.random.choice(choices[0])

    words.append(int_to_vocab[choice])
    
    for _ in range(100):
        ix = torch.tensor([[choice]]).to(device)
        output, (state_h, state_c) = net(ix, (state_h, state_c))

        _, top_ix = torch.topk(output[0], k=top_k)
        choices = top_ix.tolist()
        choice = np.random.choice(choices[0])
        words.append(int_to_vocab[choice])

    print(' '.join(words))
```


```python
main()
```

    Vocabulary size 23621
    Epoch: 0/200 Iteration: 100 Loss: 8.46151351928711
    Epoch: 1/200 Iteration: 200 Loss: 7.4185590744018555
    Epoch: 1/200 Iteration: 300 Loss: 6.699943542480469
    Epoch: 2/200 Iteration: 400 Loss: 6.079729080200195
    Epoch: 2/200 Iteration: 500 Loss: 5.269423484802246
    Epoch: 3/200 Iteration: 600 Loss: 4.745441436767578
    Epoch: 4/200 Iteration: 700 Loss: 5.041678428649902
    Epoch: 4/200 Iteration: 800 Loss: 3.514251232147217
    Epoch: 5/200 Iteration: 900 Loss: 3.830817937850952
    Epoch: 5/200 Iteration: 1000 Loss: 3.346404790878296
    I am for a profound cult group and the NT evidence: by Jesus’ ascension for a new orientation are the church is a pleasant between God and early understanding of Jesus as an insight important to understand the new covenant. It concludes the NT evidence: the Holy Spirit.—D.J.H. 976. A. Penna on 1 Thessalonians, The slightly introduction in 1 John, to serious objections. Furthermore, the University as a circle. patera. W. Hollenbach, and accepted in 1 Cov 1 1129. R. Gualtieri, in 2 Cor 2:15-3:18, Jesus’ saying is to be understood. the NT evidence. Yet a new universe. is adopted: the risen years



    ---------------------------------------------------------------------------

    FileNotFoundError                         Traceback (most recent call last)

    /tmp/ipykernel_881/451043146.py in <module>
    ----> 1 main()
    

    /tmp/ipykernel_881/947889887.py in main()
         59                 predict(device, net, flags.initial_words, n_vocab,
         60                         vocab_to_int, int_to_vocab, top_k=5)
    ---> 61                 torch.save(net.state_dict(),
         62                            'checkpoint_pt/model-{}.pth'.format(iteration))


    ~/.local/share/virtualenvs/ML-oMIlVeed/lib/python3.8/site-packages/torch/serialization.py in save(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)
        374     _check_dill_version(pickle_module)
        375 
    --> 376     with _open_file_like(f, 'wb') as opened_file:
        377         if _use_new_zipfile_serialization:
        378             with _open_zipfile_writer(opened_file) as opened_zipfile:


    ~/.local/share/virtualenvs/ML-oMIlVeed/lib/python3.8/site-packages/torch/serialization.py in _open_file_like(name_or_buffer, mode)
        228 def _open_file_like(name_or_buffer, mode):
        229     if _is_path(name_or_buffer):
    --> 230         return _open_file(name_or_buffer, mode)
        231     else:
        232         if 'w' in mode:


    ~/.local/share/virtualenvs/ML-oMIlVeed/lib/python3.8/site-packages/torch/serialization.py in __init__(self, name, mode)
        209 class _open_file(_opener):
        210     def __init__(self, name, mode):
    --> 211         super(_open_file, self).__init__(open(name, mode))
        212 
        213     def __exit__(self, *args):


    FileNotFoundError: [Errno 2] No such file or directory: 'checkpoint_pt/model-1000.pth'



```python

```
