{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "turned-atlas",
   "metadata": {},
   "source": [
    "---\n",
    "layout: default\n",
    "title: \"Preparatory concepts\"\n",
    "categories: introduction\n",
    "permalink: /ML1/\n",
    "order: 1\n",
    "comments: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "actual-polymer",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mpl_flow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0a2fa9920c8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgraphviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDigraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_flow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mpl_flow'"
     ]
    }
   ],
   "source": [
    "%pylab --no-import-all inline\n",
    "import pandas as pd\n",
    "from graphviz import Digraph\n",
    "from mpl_flow import Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-homework",
   "metadata": {},
   "source": [
    "# Derivatives and computation graph\n",
    "Derivatives are a fundamental concept in machine learning, they are the building block of optimization and having an understanding of what a derivative is vastly helps in understanding how optimization and gradient descent work.\n",
    "\n",
    "## Derivative\n",
    "Suppose we have a function $f(a) = 3a$, then $f(2) = 6$. If we take a small increment of $a$ ($a'$) we will have $f(2.001) = 6.003$. Connecting $a$ and $a'$ forms a triangle, with an height ($a'-a$) and a width ($f(a') - f(a)$) (<a href=\"#fig:derivative\">figure below</a>).\n",
    "\n",
    "The slope $\\frac{\\text{height} }{\\text{width}}=3$ so we say that the derivative of $f(a)$ at the point $a=2$ is $3$. Height and width are the the vertical and horizontal distances and the slope is also expressed as $\\frac{df(a)}{da}$ or as $\\frac{d}{da}f(a)$. The reason why $a'$ doesn't appear in this representation is because, formally, the derivative is calculated at a very small increment of $a$ such as $a' \\approx a$.\n",
    "\n",
    "For a straight line (<a href=\"#fig:derivative\">figure below</a>, panel A) the derivative is constant along the whole line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-assault",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax, ax2 = axes\n",
    "xmax = 5\n",
    "x = np.linspace(0, xmax)\n",
    "y = 3*x\n",
    "ax.plot(x, y)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(-xmax*.1, xmax*3)\n",
    "ax.plot([2, 4, 4, 2], [6, 12, 6, 6], marker='o', lw=2, markevery=[0, 1])\n",
    "\n",
    "ax.set_xticks([2, 4])\n",
    "ax.set_xticklabels([2, 2.001])\n",
    "ax.set_yticks([6, 12])\n",
    "ax.set_yticklabels([6, 6.003])\n",
    "ax.set_xlabel('')\n",
    "ax.annotate('0.001', (1.6, 5), xytext=(4.4, 5), va='center',  arrowprops={'arrowstyle': '<->'})\n",
    "ax.annotate('0.003', (5, 5.7), xytext=(5, 12.6), ha='center',  arrowprops={'arrowstyle': '<->'});\n",
    "ax.text(5.5, 9, '$df(a)$', fontsize=13)\n",
    "ax.text(3, 4.5, '$da$', va='top', ha='center', fontsize=13)\n",
    "ax.set_title('$f(a)=3a$', fontsize=13)\n",
    "ax.text(-0.1, 1.1, 'A', transform=ax.transAxes,\n",
    "  fontsize=15, va='top')\n",
    "\n",
    "ax2.plot(x, x**2)\n",
    "ax2.set_xlim(-xmax*.1, xmax*1.4)\n",
    "\n",
    "x1, x2 = 10, 15\n",
    "dx = np.r_[x[[x1, x2]], x[x2], x[x1]]\n",
    "dy = np.r_[x[[x1, x2]], x[x1], x[x1]]**2\n",
    "ax2.plot(dx, dy, marker='o', markevery=[0, 1])\n",
    "\n",
    "x1, x2 = 40, 45\n",
    "dx = np.r_[x[[x1, x2]], x[x2], x[x1]]\n",
    "dy = np.r_[x[[x1, x2]], x[x1], x[x1]]**2\n",
    "ax2.plot(dx, dy, marker='o', markevery=[0, 1])\n",
    "ax2.set_title('$f(a)=a^2$', fontsize=13)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_xticklabels([])\n",
    "ax2.set_yticks([])\n",
    "ax2.set_yticklabels([])\n",
    "ax2.text(-0.1, 1.1, 'B', transform=ax2.transAxes,\n",
    "  fontsize=15, va='top');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-killing",
   "metadata": {},
   "source": [
    "<i id=\"fig:derivative\">The concept of derivative applied to a straight line (A), where the derivative is constant along the whole length of the function; and to a non-linear function (B), where the derivative changes based on the value of $a$.</i>\n",
    "\n",
    "## Computational graph\n",
    "The computational graph explains the forward- and backward- propagation (as to say the flow of the computation) that takes place in the training of a neural network. \n",
    "\n",
    "To illustrate the computation graph let's use a simpler example than a full blown neural network, let's say that we are writing a function $J(a, b, c) = 3(a+bc)$. In order to compute this function there are three steps: \n",
    "\n",
    "1. $u = bc$\n",
    "2. $v = a + u$\n",
    "3. $J=3v$\n",
    "\n",
    "We can draw these steps in a computational graph (<a href=\"#compgraph\">figure below</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Flow()\n",
    "f.node('a', xlabel=5)\n",
    "f.node('b', travel='s', connect=False, xlabel=3)\n",
    "f.node('c', travel='s', connect=False, xlabel=2)\n",
    "f.node('u', label='$u=bc$', xlabel=6, startpoint='b')\n",
    "f.node('v', label='$v=a+u$', xlabel=11)\n",
    "f.node('j', label='$J=3v$', xlabel=33)\n",
    "f.edge('c', 'u')\n",
    "f.edge('a', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-plenty",
   "metadata": {},
   "source": [
    "<i id=\"compgraph\">Computational graph showing the flow of a very simple process</i>\n",
    "\n",
    "Suppose we want to calculate $\\frac{dJ}{dv}$ ( in other words if we change the value $v$ of a little amount how would the value of $J$ change?). \n",
    "\n",
    "* $J = 3v$\n",
    "* $v = 11 \\to 3.001$\n",
    "* $J = 33 \\to 33.003$\n",
    "\n",
    "So \n",
    "\n",
    "$$\\frac{dJ}{dv}=\\frac{0.003}{0.001}=3$$\n",
    "\n",
    "In the terminology of backpropagation if we want to compute $\\frac{dJ}{dv}$ we take one step back from $J$ to $v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-richmond",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "f = Flow()\n",
    "f.node('a', xlabel=5)\n",
    "f.node('b', travel='s', connect=False, xlabel=3)\n",
    "f.node('c', travel='s', connect=False, xlabel=2)\n",
    "f.node('u', label='$u=bc$', xlabel=6, startpoint='b')\n",
    "f.node('v', label='$v=a+u$', xlabel=11)\n",
    "f.node('j', label='$J=3v$', xlabel=33)\n",
    "f.edge('c', 'u')\n",
    "f.edge('a', 'v')\n",
    "f.edge('j', 'v', headport='se', tailport='sw', arrowprops=dict(color='red', connectionstyle='arc3,rad=.5'))\n",
    "# dot2.edge('j', 'v', headport='s', tailport='s', color='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-nicholas",
   "metadata": {},
   "source": [
    "We now want to calculate $\\frac{dJ}{da}$, in other words the change of value $J$ when $a$ changes\n",
    "\n",
    "* $a = 5 \\to 5.001$\n",
    "* $v = 11 \\to 11.001$\n",
    "* $J = 33 \\to 33.003$\n",
    "\n",
    "So, once again\n",
    "\n",
    "$$\\frac{dJ}{da}=\\frac{0.003}{0.001}=3$$\n",
    "\n",
    "Where the net change is given by \n",
    "\n",
    "$$\n",
    "\\frac{dJ}{da}=\\frac{dJ}{dv}\\frac{dv}{da}\n",
    "$$\n",
    "\n",
    "\n",
    "In calculus this is called the **chain rule** where $a$ affects $v$ that affects $J$ ($a\\to v \\to J$). So that the change of $J$ when $a$ is given by the product $\\frac{dJ}{dv}\\frac{dv}{da}$. This illustrates how having computed $\\frac{dJ}{dv}$ helps in calculating $\\frac{dJ}{da}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-raleigh",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "f = Flow()\n",
    "f.node('a', xlabel=5)\n",
    "f.node('b', travel='s', connect=False, xlabel=3)\n",
    "f.node('c', travel='s', connect=False, xlabel=2)\n",
    "f.node('u', label='$u=bc$', xlabel=6, startpoint='b')\n",
    "f.node('v', label='$v=a+u$', xlabel=11)\n",
    "f.node('j', label='$J=3v$', xlabel=33)\n",
    "f.edge('c', 'u')\n",
    "f.edge('a', 'v')\n",
    "f.edge('j', 'v', headport='se', tailport='sw', arrowprops=dict(color='red', connectionstyle='arc3,rad=.5'))\n",
    "f.edge('v', 'a', headport='e', tailport='n', arrowprops=dict(color='red', connectionstyle='arc3,rad=-.3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-petite",
   "metadata": {},
   "source": [
    "# Python vectorization\n",
    "In the pre-deep-learning era vectorization was optional, in the deep-learning era vectorization absolutely necessary since both the size of networks and of data is vastly increased.\n",
    "\n",
    "## Vector-vector product\n",
    "In particular, in deep learning (and in machine learning in general) we need to calculate \n",
    "\n",
    "$$\n",
    "z = w^Tx+b\n",
    "$$\n",
    "\n",
    "for \n",
    "\n",
    "$$\n",
    "w =\n",
    "\\begin{bmatrix}\n",
    "\\vdots \\\\ \\vdots\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n_x}\n",
    "\\qquad \n",
    "x = \\begin{bmatrix}\n",
    "\\vdots \\\\ \\vdots\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n_x}\n",
    "$$\n",
    "\n",
    "The vectorized form of this operation in python is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-raising",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "w, x, b = np.random.rand(3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-multiple",
   "metadata": {
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "np.dot(w, x) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-growing",
   "metadata": {},
   "source": [
    "where `np.dot(w, x)` $\\equiv w^Tx$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-amount",
   "metadata": {},
   "source": [
    "## Matrix-vector product\n",
    "Incidentally, the matrix-vector product $Av$, where \n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "\\ddots &  \\\\\n",
    "&   \\\\\n",
    "&  \\ddots \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m \\times n} \\qquad \n",
    "v=\\begin{bmatrix}\n",
    "\\vdots \\\\ \\vdots\n",
    "\\end{bmatrix} \\in \\mathbb{R}^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-senator",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "v = np.random.rand(10)\n",
    "A = np.random.rand(3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-parking",
   "metadata": {
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "np.dot(A, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-rotation",
   "metadata": {},
   "source": [
    "Notice that the exact same syntax performs both vecto-vector and matrix-vector multiplication, this is due to the overload implemented in the `np.dot` function. To know more about it, check out [its documentation](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)\n",
    "\n",
    "## Vectorized element-wise operations\n",
    "To apply a function element by element to whole arrays you can simply use`np.ufuncs` ([numpy universal functions](https://numpy.org/doc/stable/reference/generated/numpy.ufunc.html#numpy.ufunc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-department",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "v = np.random.rand(10).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(v).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(v).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "v + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "v * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-commerce",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "To a complete guide to broadcasting check out [numpy great documentation](https://numpy.org/doc/stable/user/basics.broadcasting.html#:~:text=The%20term%20broadcasting%20describes%20how,that%20they%20have%20compatible%20shapes.&text=NumPy%20operations%20are%20usually%20done,element%2Dby%2Delement%20basis.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.DataFrame([[56, 0, 4.4, 6.8], [1.2, 104, 52, 8], [1.8, 135, 99, 0.9]], \n",
    "                        columns=['Apples', 'Beef', 'Eggs', 'Potatoes'], index=['Carb', 'Protein', 'Fat'])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A.values\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = A.sum(axis=0)\n",
    "cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "(A / cal.reshape(1, 4) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "A / cal * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-efficiency",
   "metadata": {},
   "source": [
    "In general if you have a $m, n$ matrix (A) \n",
    "\n",
    "* if you apply an operation with an $1, n$ matrix (B), then B will be copied $m$ times and the operations applied element-wise\n",
    "* if you apply an operation with an $m, 1$ matrix (C), then C will be copied $n$ times and the operations applied element-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-hayes",
   "metadata": {},
   "source": [
    "## numpy Vectors\n",
    "`numpy` offers great flexibility at the cost of rigorousness, sometimes wrong-looking expression give unexpectedly correct results and vice versa.\n",
    "Heres a series of considerations and suggestions for dealing with `numpy`.\n",
    "\n",
    "For example let's take a random vector of 5 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-telling",
   "metadata": {},
   "source": [
    "Whose shape is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-silence",
   "metadata": {},
   "source": [
    "This is called a rank 1 vector in python and it's neither a row vector nor a column vector and its behavior is sometimes unexpected. \n",
    "\n",
    "For example, its transpose is equal to itself "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-webster",
   "metadata": {},
   "source": [
    "and the inner product of `a` and `a.T` is not a matrix instead is a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(a, a.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-bracelet",
   "metadata": {},
   "source": [
    "So, instead of using rank 1 vectors you may want to use rank 2 vectors, which have a much more predictable behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(5, 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(a, a.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-elements",
   "metadata": {},
   "source": [
    "rank 1 arrays can always be reshaped in row or columns vectors (or higher dimensional matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.reshape(5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.reshape(1, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
